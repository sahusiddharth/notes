{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Link: https://huggingface.co/datasets/allenai/qasper\n",
    "\n",
    "While leaderboards and reports offer insights into overall model performance, they don't reveal how a model handles your specific needs. The Gen AI evaluation service helps you define your own evaluation criteria, ensuring a clear understanding of how well generative AI models and applications align with your unique use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-genai -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Gemini Models on using Ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will see how we can benchmark the Gemini models on the AllenAI's QASPER dataset using the RAGAS metrics on Question Answering Task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data collection Process of QASPER Dataset](qasper_data_collection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "\n",
    "For the sake of demonstration, we will use only a subset of the whole dataset. You can perform benchmarking using the complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'abstract', 'full_text', 'qas', 'figures_and_tables'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "dataset = load_dataset(\"allenai/qasper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](benchmarking.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_full_text_to_markdown(full_text_dict):\n",
    "    \"\"\"\n",
    "    Converts a full_text dictionary into a markdown-formatted string.\n",
    "\n",
    "    Expected keys:\n",
    "      - \"section_name\": list of section titles.\n",
    "      - \"paragraphs\": list of lists of paragraphs corresponding to each section.\n",
    "\n",
    "    Each section becomes a markdown header (##) followed by its paragraphs.\n",
    "    \"\"\"\n",
    "    sections = full_text_dict.get(\"section_name\", [])\n",
    "    paragraphs = full_text_dict.get(\"paragraphs\", [])\n",
    "\n",
    "    markdown_lines = []\n",
    "    for section, paragraph in zip(sections, paragraphs):\n",
    "        markdown_lines.append(f\"## {section}\")\n",
    "        markdown_lines.append(\"\")  # Blank line\n",
    "        markdown_lines.append(\"\\n\".join(map(str, paragraph)))\n",
    "        markdown_lines.append(\"\")  # End of section\n",
    "        markdown_lines.append(\"\")  # Extra blank line for separation\n",
    "    return \"\\n\".join(markdown_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_responses(row):\n",
    "    \"\"\"\n",
    "    Combines 'extractive_spans', 'yes_no', and 'free_form_answer'\n",
    "    into one single string. Skips components that are missing.\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "    if pd.notna(row.get(\"extractive_spans\")):\n",
    "        if isinstance(row[\"extractive_spans\"], list):\n",
    "            responses.append(\" \".join(map(str, row[\"extractive_spans\"])))\n",
    "        else:\n",
    "            responses.append(str(row[\"extractive_spans\"]))\n",
    "    if pd.notna(row.get(\"yes_no\")):\n",
    "        responses.append(str(row[\"yes_no\"]))\n",
    "    if pd.notna(row.get(\"free_form_answer\")):\n",
    "        responses.append(str(row[\"free_form_answer\"]))\n",
    "    return \"\\n\".join(responses) if responses else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_hf_dataset(hf_ds):\n",
    "    \"\"\"\n",
    "    Processes a HuggingFace dataset split into a cleaned Pandas DataFrame.\n",
    "\n",
    "    Steps:\n",
    "      1. For each sample, convert 'full_text' to a markdown string.\n",
    "      2. For every QA pair in the sample, extract the question and first answer.\n",
    "      3. Build lists for answers, questions, and full_text (duplicated per question).\n",
    "      4. Create a DataFrame from the collected data.\n",
    "      5. Clean columns by replacing empty lists/strings with NaN and joining lists.\n",
    "      6. Combine the answer components into a single 'golden response'.\n",
    "\n",
    "    The function uses nested tqdm progress bars for real-time feedback.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "    answers_list = []  # Stores the first answer for each question\n",
    "    questions_list = []  # Stores each question text\n",
    "    full_text_list = []  # Stores the formatted full text per QA pair\n",
    "\n",
    "    # Outer loop: iterate over samples with progress bar\n",
    "    for sample in tqdm(hf_ds, desc=\"Processing samples\", unit=\"sample\"):\n",
    "        # Convert full text once per sample\n",
    "        formatted_text = convert_full_text_to_markdown(sample[\"full_text\"])\n",
    "        # Create a list of QA pairs\n",
    "        qa_pairs = list(zip(sample[\"qas\"][\"question\"], sample[\"qas\"][\"answers\"]))\n",
    "\n",
    "        # Inner loop: iterate over each QA pair with its own progress bar\n",
    "        for question, answer_set in tqdm(\n",
    "            qa_pairs, desc=\"Processing QAs\", total=len(qa_pairs), leave=False, unit=\"qa\"\n",
    "        ):\n",
    "            answers_list.append(answer_set[\"answer\"][0])\n",
    "            questions_list.append(question)\n",
    "            full_text_list.append(formatted_text)\n",
    "\n",
    "    # Create DataFrame from the collected data\n",
    "    df = pd.DataFrame(answers_list)\n",
    "    df[\"question\"] = questions_list\n",
    "    df[\"full_text\"] = full_text_list\n",
    "\n",
    "    # Data Cleaning: Replace empty lists/strings with NaN and join lists if needed\n",
    "    df[\"extractive_spans\"] = df[\"extractive_spans\"].apply(\n",
    "        lambda x: np.nan if isinstance(x, list) and len(x) == 0 else x\n",
    "    )\n",
    "    df[\"free_form_answer\"] = df[\"free_form_answer\"].apply(\n",
    "        lambda x: np.nan if isinstance(x, str) and x.strip() == \"\" else x\n",
    "    )\n",
    "    df[\"yes_no\"] = df[\"yes_no\"].apply(lambda x: np.nan if x is None else x)\n",
    "    df[\"extractive_spans\"] = df[\"extractive_spans\"].apply(\n",
    "        lambda x: \"\\n\".join(x) if isinstance(x, list) else x\n",
    "    )\n",
    "\n",
    "    # Combine the answer components into a single 'golden response'\n",
    "    df[\"golden response\"] = df.apply(lambda row: combine_responses(row), axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dataset[\"train\"]\n",
    "validation_ds = dataset[\"validation\"]\n",
    "test_ds = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples: 100%|██████████| 888/888 [00:04<00:00, 211.20sample/s]\n",
      "Processing samples: 100%|██████████| 281/281 [00:01<00:00, 199.77sample/s]\n",
      "Processing samples: 100%|██████████| 416/416 [00:02<00:00, 198.84sample/s]\n"
     ]
    }
   ],
   "source": [
    "train_df = preprocess_hf_dataset(train_ds)\n",
    "validation_df = preprocess_hf_dataset(validation_ds)\n",
    "test_df = preprocess_hf_dataset(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "gemini_2 = GoogleGenAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI learns patterns from data to make predictions or decisions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\", contents=\"Explain how AI works in a few words\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'which multilingual approaches do they compare with?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "query = validation_df.iloc[idx][\"question\"]\n",
    "context = validation_df.iloc[idx][\"full_text\"]\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The paper compares its approach with multilingual NMT (MNMT) from  BIBREF19.  Another comparison is made against a pivoting method that uses MNMT (pivoting<sub>m</sub>), which uses MNMT to translate source to pivot and then to target.\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_str = context\n",
    "query_str = query\n",
    "\n",
    "\n",
    "qa_prompt = (\n",
    "    f\"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the query.\\n\"\n",
    "    \"If you cannot answer the query, just say that it cannot be answered.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "formatted_prompt = qa_prompt.format(context_str=context_str, query_str=query_str)\n",
    "response = gemini_2.complete(formatted_prompt)\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They compare their approaches with Multilingual NMT (MNMT) described in BIBREF19 and BIBREF22.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "context_str = context\n",
    "query_str = query\n",
    "\n",
    "qa_prompt = (\n",
    "    f\"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the query.\\n\"\n",
    "    \"If you cannot answer the query, just say that it cannot be answered.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "formatted_prompt = qa_prompt.format(context_str=context_str, query_str=query_str)\n",
    "\n",
    "\n",
    "response = await client.aio.models.generate_content(\n",
    "    model='gemini-2.0-flash', contents=formatted_prompt\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BIBREF19\\nBIBREF20'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df.iloc[idx][\"golden response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from async_executor import AsyncExecutor\n",
    "\n",
    "gemini_2 = GoogleGenAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    ")\n",
    "\n",
    "async def query_llm(query_str: str, context_str: str):\n",
    "    formatted_prompt = qa_prompt.format(context_str=context_str, query_str=query_str)\n",
    "    response = await gemini_2.acomplete(formatted_prompt)\n",
    "    return response\n",
    "\n",
    "\n",
    "# Create an instance of the asynchronous executor\n",
    "executor = AsyncExecutor(\n",
    "    desc=\"LLM Processing\",\n",
    "    show_progress=True,\n",
    "    raise_exceptions=False,\n",
    "    max_calls_per_minute=1250,\n",
    ")\n",
    "\n",
    "df = validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Processing: 100%|██████████| 1005/1005 [00:52<00:00, 19.13it/s]\n"
     ]
    }
   ],
   "source": [
    "for idx in range(df.shape[0]):\n",
    "    query = df.iloc[idx][\"question\"]\n",
    "    context = df.iloc[idx][\"full_text\"]\n",
    "    executor.submit(query_llm, query, context)\n",
    "\n",
    "# Execute the jobs and get the results in order\n",
    "validation_responses = executor.results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>which multilingual approaches do they compare ...</td>\n",
       "      <td>They compare their approaches with Multilingua...</td>\n",
       "      <td>BIBREF19\\nBIBREF20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what are the pivot-based baselines?</td>\n",
       "      <td>The pivot-based method is used as a baseline. ...</td>\n",
       "      <td>pivoting\\npivoting$_{\\rm m}$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>which datasets did they experiment with?</td>\n",
       "      <td>They experimented with two public datasets: Eu...</td>\n",
       "      <td>Europarl\\nMultiUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what language pairs are explored?</td>\n",
       "      <td>The language pairs explored in this paper are:...</td>\n",
       "      <td>De-En, En-Fr, Fr-En, En-Es, Ro-En, En-De, Ar-E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what ner models were evaluated?</td>\n",
       "      <td>Stanford NER, spaCy 2.0, and a recurrent model...</td>\n",
       "      <td>Stanford NER\\nspaCy 2.0 \\nrecurrent model with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>What approaches do they use towards text analy...</td>\n",
       "      <td>Based on the provided text, the approaches use...</td>\n",
       "      <td>Domain experts and fellow researchers can prov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>What dataset do they use for analysis?</td>\n",
       "      <td>The context information mentions using data fr...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>Do they demonstrate why interdisciplinary insi...</td>\n",
       "      <td>Yes, the text explicitly states that interdisc...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>What background do they have?</td>\n",
       "      <td>The authors are scholars from very different d...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>What kind of issues (that are not on the foref...</td>\n",
       "      <td>The article aims to shed light on thorny issue...</td>\n",
       "      <td>identifying the questions we wish to explore\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1005 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             user_input  \\\n",
       "0     which multilingual approaches do they compare ...   \n",
       "1                   what are the pivot-based baselines?   \n",
       "2              which datasets did they experiment with?   \n",
       "3                     what language pairs are explored?   \n",
       "4                       what ner models were evaluated?   \n",
       "...                                                 ...   \n",
       "1000  What approaches do they use towards text analy...   \n",
       "1001             What dataset do they use for analysis?   \n",
       "1002  Do they demonstrate why interdisciplinary insi...   \n",
       "1003                      What background do they have?   \n",
       "1004  What kind of issues (that are not on the foref...   \n",
       "\n",
       "                                               response  \\\n",
       "0     They compare their approaches with Multilingua...   \n",
       "1     The pivot-based method is used as a baseline. ...   \n",
       "2     They experimented with two public datasets: Eu...   \n",
       "3     The language pairs explored in this paper are:...   \n",
       "4     Stanford NER, spaCy 2.0, and a recurrent model...   \n",
       "...                                                 ...   \n",
       "1000  Based on the provided text, the approaches use...   \n",
       "1001  The context information mentions using data fr...   \n",
       "1002  Yes, the text explicitly states that interdisc...   \n",
       "1003  The authors are scholars from very different d...   \n",
       "1004  The article aims to shed light on thorny issue...   \n",
       "\n",
       "                                              reference  \n",
       "0                                    BIBREF19\\nBIBREF20  \n",
       "1                          pivoting\\npivoting$_{\\rm m}$  \n",
       "2                                     Europarl\\nMultiUN  \n",
       "3     De-En, En-Fr, Fr-En, En-Es, Ro-En, En-De, Ar-E...  \n",
       "4     Stanford NER\\nspaCy 2.0 \\nrecurrent model with...  \n",
       "...                                                 ...  \n",
       "1000  Domain experts and fellow researchers can prov...  \n",
       "1001                                                     \n",
       "1002                                              False  \n",
       "1003                                                     \n",
       "1004  identifying the questions we wish to explore\\n...  \n",
       "\n",
       "[1005 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.dataset_schema import EvaluationDataset\n",
    "\n",
    "dataset_list = []\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    sample = {\n",
    "        \"user_input\": (\n",
    "            \"\" if pd.isna(df.iloc[i].get(\"question\")) else df.iloc[i].get(\"question\")\n",
    "        ),\n",
    "        \"reference\": (\n",
    "            \"\"\n",
    "            if pd.isna(df.iloc[i].get(\"golden response\"))\n",
    "            else df.iloc[i].get(\"golden response\")\n",
    "        ),\n",
    "        \"response\": (\n",
    "            \"\"\n",
    "            if pd.isna(validation_responses[i].text)\n",
    "            else validation_responses[i].text\n",
    "        ),\n",
    "    }\n",
    "    dataset_list.append(sample)\n",
    "\n",
    "dataset = EvaluationDataset.from_list(dataset_list)\n",
    "dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    AnswerAccuracy,\n",
    "    AnswerCorrectness,\n",
    "    FactualCorrectness,\n",
    "    AspectCritic,\n",
    ")\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "\n",
    "aspect_critic = AspectCritic(\n",
    "    name=\"unanswerable\",\n",
    "    definition=\"Return 1 if the query cannot be answered by the provided context, otherwise return 0.\",\n",
    "    llm=evaluator_llm,\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    AnswerAccuracy(llm=evaluator_llm),\n",
    "    AnswerCorrectness(llm=evaluator_llm, weights=[1, 0]),\n",
    "    aspect_critic,\n",
    "    FactualCorrectness(llm=evaluator_llm),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'They compare their approaches with Multilingual NMT (MNMT) from BIBREF19 and BIBREF22.\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_responses[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4020/4020 [25:38<00:00,  2.61it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>nv_accuracy</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>unanswerable</th>\n",
       "      <th>factual_correctness(mode=f1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>which multilingual approaches do they compare ...</td>\n",
       "      <td>They compare their approaches with Multilingua...</td>\n",
       "      <td>BIBREF19\\nBIBREF20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what are the pivot-based baselines?</td>\n",
       "      <td>The pivot-based method is used as a baseline. ...</td>\n",
       "      <td>pivoting\\npivoting$_{\\rm m}$</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>which datasets did they experiment with?</td>\n",
       "      <td>They experimented with two public datasets: Eu...</td>\n",
       "      <td>Europarl\\nMultiUN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what language pairs are explored?</td>\n",
       "      <td>The language pairs explored in this paper are:...</td>\n",
       "      <td>De-En, En-Fr, Fr-En, En-Es, Ro-En, En-De, Ar-E...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what ner models were evaluated?</td>\n",
       "      <td>Stanford NER, spaCy 2.0, and a recurrent model...</td>\n",
       "      <td>Stanford NER\\nspaCy 2.0 \\nrecurrent model with...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>What approaches do they use towards text analy...</td>\n",
       "      <td>Based on the provided text, the approaches use...</td>\n",
       "      <td>Domain experts and fellow researchers can prov...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>What dataset do they use for analysis?</td>\n",
       "      <td>The context information mentions using data fr...</td>\n",
       "      <td></td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>Do they demonstrate why interdisciplinary insi...</td>\n",
       "      <td>Yes, the text explicitly states that interdisc...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>What background do they have?</td>\n",
       "      <td>The authors are scholars from very different d...</td>\n",
       "      <td></td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>What kind of issues (that are not on the foref...</td>\n",
       "      <td>The article aims to shed light on thorny issue...</td>\n",
       "      <td>identifying the questions we wish to explore\\n...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1005 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             user_input  \\\n",
       "0     which multilingual approaches do they compare ...   \n",
       "1                   what are the pivot-based baselines?   \n",
       "2              which datasets did they experiment with?   \n",
       "3                     what language pairs are explored?   \n",
       "4                       what ner models were evaluated?   \n",
       "...                                                 ...   \n",
       "1000  What approaches do they use towards text analy...   \n",
       "1001             What dataset do they use for analysis?   \n",
       "1002  Do they demonstrate why interdisciplinary insi...   \n",
       "1003                      What background do they have?   \n",
       "1004  What kind of issues (that are not on the foref...   \n",
       "\n",
       "                                               response  \\\n",
       "0     They compare their approaches with Multilingua...   \n",
       "1     The pivot-based method is used as a baseline. ...   \n",
       "2     They experimented with two public datasets: Eu...   \n",
       "3     The language pairs explored in this paper are:...   \n",
       "4     Stanford NER, spaCy 2.0, and a recurrent model...   \n",
       "...                                                 ...   \n",
       "1000  Based on the provided text, the approaches use...   \n",
       "1001  The context information mentions using data fr...   \n",
       "1002  Yes, the text explicitly states that interdisc...   \n",
       "1003  The authors are scholars from very different d...   \n",
       "1004  The article aims to shed light on thorny issue...   \n",
       "\n",
       "                                              reference  nv_accuracy  \\\n",
       "0                                    BIBREF19\\nBIBREF20         0.25   \n",
       "1                          pivoting\\npivoting$_{\\rm m}$         0.50   \n",
       "2                                     Europarl\\nMultiUN         1.00   \n",
       "3     De-En, En-Fr, Fr-En, En-Es, Ro-En, En-De, Ar-E...         0.00   \n",
       "4     Stanford NER\\nspaCy 2.0 \\nrecurrent model with...         0.50   \n",
       "...                                                 ...          ...   \n",
       "1000  Domain experts and fellow researchers can prov...         0.00   \n",
       "1001                                                            0.50   \n",
       "1002                                              False         0.00   \n",
       "1003                                                            0.50   \n",
       "1004  identifying the questions we wish to explore\\n...         0.25   \n",
       "\n",
       "      answer_correctness  unanswerable  factual_correctness(mode=f1)  \n",
       "0                    0.5             0                          0.67  \n",
       "1                    0.8             0                          0.00  \n",
       "2                    0.8             0                          0.40  \n",
       "3                    1.0             0                          0.00  \n",
       "4                    0.8             0                          0.00  \n",
       "...                  ...           ...                           ...  \n",
       "1000                 0.0             0                          0.00  \n",
       "1001                 0.0             0                          0.00  \n",
       "1002                 0.0             0                          0.00  \n",
       "1003                 0.0             1                          0.00  \n",
       "1004                 0.0             0                          0.00  \n",
       "\n",
       "[1005 rows x 7 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "\n",
    "gemini_2_score = evaluate(dataset=dataset, metrics=metrics)\n",
    "gemini_2_score.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A completely optional step, if you want to upload the evaluation results to your Ragas app, you can run the command below.You can learn more about Ragas app here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results uploaded! View at https://app.ragas.io/dashboard/alignment/evaluation/908c34a5-3996-4703-8eae-a7daf210c6d7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://app.ragas.io/dashboard/alignment/evaluation/908c34a5-3996-4703-8eae-a7daf210c6d7'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_2_score.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = gemini_2_score[\"unanswerable\"]\n",
    "actuals = validation_df[\"unanswerable\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.844776119402985\n",
      "Precision: 0.31736526946107785\n",
      "Recall: 0.5578947368421052\n",
      "F1 Score: 0.40458015267175573\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.87      0.91       910\n",
      "           1       0.32      0.56      0.40        95\n",
      "\n",
      "    accuracy                           0.84      1005\n",
      "   macro avg       0.63      0.72      0.66      1005\n",
      "weighted avg       0.89      0.84      0.86      1005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "# Calculate and print basic metrics\n",
    "print(\"Accuracy:\", accuracy_score(actuals, preds))\n",
    "print(\"Precision:\", precision_score(actuals, preds))\n",
    "print(\"Recall:\", recall_score(actuals, preds))\n",
    "print(\"F1 Score:\", f1_score(actuals, preds))\n",
    "\n",
    "# Generate and print the classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(actuals, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking Gemini 1.5 Flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_1_5 = GoogleGenAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    ")\n",
    "\n",
    "\n",
    "async def query_llm(query_str: str, context_str: str):\n",
    "    formatted_prompt = qa_prompt.format(context_str=context_str, query_str=query_str)\n",
    "    response = await gemini_1_5.acomplete(formatted_prompt)\n",
    "    return response\n",
    "\n",
    "\n",
    "# Create an instance of the asynchronous executor\n",
    "executor = AsyncExecutor(\n",
    "    desc=\"Querying LLM\",\n",
    "    show_progress=True,\n",
    "    raise_exceptions=False,\n",
    "    max_calls_per_minute=1250,\n",
    ")\n",
    "\n",
    "df = validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(df.shape[0]):\n",
    "    query = df.iloc[idx][\"question\"]\n",
    "    context = df.iloc[idx][\"full_text\"]\n",
    "    executor.submit(query_llm, query, context)\n",
    "\n",
    "# Execute the jobs and get the results in order\n",
    "validation_responses = executor.results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>which multilingual approaches do they compare ...</td>\n",
       "      <td>The paper compares its approach with multiling...</td>\n",
       "      <td>BIBREF19\\nBIBREF20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what are the pivot-based baselines?</td>\n",
       "      <td>The provided text mentions two types of pivot-...</td>\n",
       "      <td>pivoting\\npivoting$_{\\rm m}$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>which datasets did they experiment with?</td>\n",
       "      <td>The experiments were conducted on two public d...</td>\n",
       "      <td>Europarl\\nMultiUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what language pairs are explored?</td>\n",
       "      <td>The paper explores the following language pair...</td>\n",
       "      <td>De-En, En-Fr, Fr-En, En-Es, Ro-En, En-De, Ar-E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what ner models were evaluated?</td>\n",
       "      <td>Stanford NER, spaCy 2.0, and a recurrent model...</td>\n",
       "      <td>Stanford NER\\nspaCy 2.0 \\nrecurrent model with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>What approaches do they use towards text analy...</td>\n",
       "      <td>The authors utilize several approaches to text...</td>\n",
       "      <td>Domain experts and fellow researchers can prov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>What dataset do they use for analysis?</td>\n",
       "      <td>The primary dataset used for analysis in the p...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>Do they demonstrate why interdisciplinary insi...</td>\n",
       "      <td>Yes, the authors demonstrate the importance of...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>What background do they have?</td>\n",
       "      <td>The authors have diverse disciplinary backgrou...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>What kind of issues (that are not on the foref...</td>\n",
       "      <td>The authors tackle thorny issues related to th...</td>\n",
       "      <td>identifying the questions we wish to explore\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1005 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             user_input  \\\n",
       "0     which multilingual approaches do they compare ...   \n",
       "1                   what are the pivot-based baselines?   \n",
       "2              which datasets did they experiment with?   \n",
       "3                     what language pairs are explored?   \n",
       "4                       what ner models were evaluated?   \n",
       "...                                                 ...   \n",
       "1000  What approaches do they use towards text analy...   \n",
       "1001             What dataset do they use for analysis?   \n",
       "1002  Do they demonstrate why interdisciplinary insi...   \n",
       "1003                      What background do they have?   \n",
       "1004  What kind of issues (that are not on the foref...   \n",
       "\n",
       "                                               response  \\\n",
       "0     The paper compares its approach with multiling...   \n",
       "1     The provided text mentions two types of pivot-...   \n",
       "2     The experiments were conducted on two public d...   \n",
       "3     The paper explores the following language pair...   \n",
       "4     Stanford NER, spaCy 2.0, and a recurrent model...   \n",
       "...                                                 ...   \n",
       "1000  The authors utilize several approaches to text...   \n",
       "1001  The primary dataset used for analysis in the p...   \n",
       "1002  Yes, the authors demonstrate the importance of...   \n",
       "1003  The authors have diverse disciplinary backgrou...   \n",
       "1004  The authors tackle thorny issues related to th...   \n",
       "\n",
       "                                              reference  \n",
       "0                                    BIBREF19\\nBIBREF20  \n",
       "1                          pivoting\\npivoting$_{\\rm m}$  \n",
       "2                                     Europarl\\nMultiUN  \n",
       "3     De-En, En-Fr, Fr-En, En-Es, Ro-En, En-De, Ar-E...  \n",
       "4     Stanford NER\\nspaCy 2.0 \\nrecurrent model with...  \n",
       "...                                                 ...  \n",
       "1000  Domain experts and fellow researchers can prov...  \n",
       "1001                                                     \n",
       "1002                                              False  \n",
       "1003                                                     \n",
       "1004  identifying the questions we wish to explore\\n...  \n",
       "\n",
       "[1005 rows x 3 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.dataset_schema import EvaluationDataset\n",
    "\n",
    "dataset_list = []\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    sample = {\n",
    "        \"user_input\": (\n",
    "            \"\" if pd.isna(df.iloc[i].get(\"question\")) else df.iloc[i].get(\"question\")\n",
    "        ),\n",
    "        \"reference\": (\n",
    "            \"\"\n",
    "            if pd.isna(df.iloc[i].get(\"golden response\"))\n",
    "            else df.iloc[i].get(\"golden response\")\n",
    "        ),\n",
    "        \"response\": (\n",
    "            \"\"\n",
    "            if pd.isna(validation_responses[i].text)\n",
    "            else validation_responses[i].text\n",
    "        ),\n",
    "    }\n",
    "    dataset_list.append(sample)\n",
    "\n",
    "dataset = EvaluationDataset.from_list(dataset_list)\n",
    "dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4020/4020 [27:40<00:00,  2.42it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>nv_accuracy</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>unanswerable</th>\n",
       "      <th>factual_correctness(mode=f1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>which multilingual approaches do they compare ...</td>\n",
       "      <td>The paper compares its approach with multiling...</td>\n",
       "      <td>BIBREF19\\nBIBREF20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what are the pivot-based baselines?</td>\n",
       "      <td>The provided text mentions two types of pivot-...</td>\n",
       "      <td>pivoting\\npivoting$_{\\rm m}$</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>which datasets did they experiment with?</td>\n",
       "      <td>The experiments were conducted on two public d...</td>\n",
       "      <td>Europarl\\nMultiUN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what language pairs are explored?</td>\n",
       "      <td>The paper explores the following language pair...</td>\n",
       "      <td>De-En, En-Fr, Fr-En, En-Es, Ro-En, En-De, Ar-E...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what ner models were evaluated?</td>\n",
       "      <td>Stanford NER, spaCy 2.0, and a recurrent model...</td>\n",
       "      <td>Stanford NER\\nspaCy 2.0 \\nrecurrent model with...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>What approaches do they use towards text analy...</td>\n",
       "      <td>The authors utilize several approaches to text...</td>\n",
       "      <td>Domain experts and fellow researchers can prov...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>What dataset do they use for analysis?</td>\n",
       "      <td>The primary dataset used for analysis in the p...</td>\n",
       "      <td></td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>Do they demonstrate why interdisciplinary insi...</td>\n",
       "      <td>Yes, the authors demonstrate the importance of...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>What background do they have?</td>\n",
       "      <td>The authors have diverse disciplinary backgrou...</td>\n",
       "      <td></td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>What kind of issues (that are not on the foref...</td>\n",
       "      <td>The authors tackle thorny issues related to th...</td>\n",
       "      <td>identifying the questions we wish to explore\\n...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1005 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             user_input  \\\n",
       "0     which multilingual approaches do they compare ...   \n",
       "1                   what are the pivot-based baselines?   \n",
       "2              which datasets did they experiment with?   \n",
       "3                     what language pairs are explored?   \n",
       "4                       what ner models were evaluated?   \n",
       "...                                                 ...   \n",
       "1000  What approaches do they use towards text analy...   \n",
       "1001             What dataset do they use for analysis?   \n",
       "1002  Do they demonstrate why interdisciplinary insi...   \n",
       "1003                      What background do they have?   \n",
       "1004  What kind of issues (that are not on the foref...   \n",
       "\n",
       "                                               response  \\\n",
       "0     The paper compares its approach with multiling...   \n",
       "1     The provided text mentions two types of pivot-...   \n",
       "2     The experiments were conducted on two public d...   \n",
       "3     The paper explores the following language pair...   \n",
       "4     Stanford NER, spaCy 2.0, and a recurrent model...   \n",
       "...                                                 ...   \n",
       "1000  The authors utilize several approaches to text...   \n",
       "1001  The primary dataset used for analysis in the p...   \n",
       "1002  Yes, the authors demonstrate the importance of...   \n",
       "1003  The authors have diverse disciplinary backgrou...   \n",
       "1004  The authors tackle thorny issues related to th...   \n",
       "\n",
       "                                              reference  nv_accuracy  \\\n",
       "0                                    BIBREF19\\nBIBREF20         0.25   \n",
       "1                          pivoting\\npivoting$_{\\rm m}$         0.25   \n",
       "2                                     Europarl\\nMultiUN         1.00   \n",
       "3     De-En, En-Fr, Fr-En, En-Es, Ro-En, En-De, Ar-E...         0.00   \n",
       "4     Stanford NER\\nspaCy 2.0 \\nrecurrent model with...         0.50   \n",
       "...                                                 ...          ...   \n",
       "1000  Domain experts and fellow researchers can prov...         0.00   \n",
       "1001                                                            1.00   \n",
       "1002                                              False         0.00   \n",
       "1003                                                            0.75   \n",
       "1004  identifying the questions we wish to explore\\n...         0.00   \n",
       "\n",
       "      answer_correctness  unanswerable  factual_correctness(mode=f1)  \n",
       "0               0.000000             0                           0.0  \n",
       "1               0.500000             0                           0.0  \n",
       "2               1.000000             0                           0.0  \n",
       "3               0.250000             0                           0.0  \n",
       "4               0.571429             0                           0.0  \n",
       "...                  ...           ...                           ...  \n",
       "1000            0.000000             0                           0.0  \n",
       "1001            0.000000             0                           0.0  \n",
       "1002            0.000000             0                           0.0  \n",
       "1003            0.000000             0                           0.0  \n",
       "1004            0.000000             0                           0.0  \n",
       "\n",
       "[1005 rows x 7 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "\n",
    "gemini_1_5_score = evaluate(dataset=dataset, metrics=metrics)\n",
    "gemini_1_5_score.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nv_accuracy': 0.4724, 'answer_correctness': 0.3366, 'unanswerable': 0.1841, 'factual_correctness(mode=f1)': 0.2269}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_1_5_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = gemini_1_5_score[\"unanswerable\"]\n",
    "actuals = validation_df[\"unanswerable\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83681592039801\n",
      "Precision: 0.31351351351351353\n",
      "Recall: 0.6105263157894737\n",
      "F1 Score: 0.4142857142857143\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.91       910\n",
      "           1       0.31      0.61      0.41        95\n",
      "\n",
      "    accuracy                           0.84      1005\n",
      "   macro avg       0.63      0.74      0.66      1005\n",
      "weighted avg       0.89      0.84      0.86      1005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "# Calculate and print basic metrics\n",
    "print(\"Accuracy:\", accuracy_score(actuals, preds))\n",
    "print(\"Precision:\", precision_score(actuals, preds))\n",
    "print(\"Recall:\", recall_score(actuals, preds))\n",
    "print(\"F1 Score:\", f1_score(actuals, preds))\n",
    "\n",
    "# Generate and print the classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(actuals, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results uploaded! View at https://app.ragas.io/dashboard/alignment/evaluation/2a3849ff-b142-4440-9c13-42f5fda332c9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://app.ragas.io/dashboard/alignment/evaluation/2a3849ff-b142-4440-9c13-42f5fda332c9'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_1_5_score.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you follow the steps like above you can Benchmark any model using Ragas Metrics you will you need to figure out How to convert the benchmark dataset to ragas EvaluationDataset then select the metrics of you choice and using then use the evaluate function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fixci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
